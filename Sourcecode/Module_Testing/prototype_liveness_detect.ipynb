{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469a1b56-bf20-4c4b-a6ec-cb03f9dcb7dc",
   "metadata": {
    "gather": {
     "logged": 1678169462854
    }
   },
   "outputs": [],
   "source": [
    "### Libraries for webcam showing in ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6a4e96-420d-4a9a-b738-df728baf8f73",
   "metadata": {
    "gather": {
     "logged": 1678169472554
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Video, Image\n",
    "from IPython.display import display\n",
    "import IPython\n",
    "import numpy as np\n",
    "import cv2\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae46182c-9827-4ddb-8d5d-10442a63641b",
   "metadata": {
    "gather": {
     "logged": 1678169475455
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b249dc-c45f-4cc6-b0be-c052dff690ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test RTSP Cam ( Camera over Network)\n",
    "\n",
    "To proof that system able to run in real condition\\\n",
    "The testing of web cam under headless server (without GUI) will be tested.\\\n",
    "Because CV2 window can't show directly through webpage.\n",
    "\n",
    "The tutorial that runs cv2 in matplotlib graph are referred:\\\n",
    "[Link to reference](https://github.com/bikz05/ipython-notebooks/blob/master/computer-vision/displaying-video-in-ipython-notebook.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03750589-c0f7-4cde-a367-ea92d2b5ef1a",
   "metadata": {
    "gather": {
     "logged": 1678169503071
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n"
     ]
    }
   ],
   "source": [
    "# Grab the input device, in this case the webcam\n",
    "\n",
    "# You can also give path to the video file\n",
    "vid = cv2.VideoCapture(\"face-direction-2.mp4\")\n",
    "\n",
    "\n",
    "# Reserve for webcam source\n",
    "### vid = cv2.VideoCapture(0)\n",
    "\n",
    "# Put the code in try-except statements\n",
    "# Catch the keyboard exception and \n",
    "# release the camera device and \n",
    "# continue with the rest of code.\n",
    "try:\n",
    "    # Check if the webcam is opened correctly\n",
    "    if not vid.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            # Release the Video Device if ret is false\n",
    "            vid.release()\n",
    "            # Message to be displayed after releasing the device\n",
    "            print(\"Released Video Resource\")\n",
    "            break\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # to display the image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Turn off the axis\n",
    "        axis('off')\n",
    "        # Title of the window\n",
    "        title(\"Input Stream\")\n",
    "        # Display the frame\n",
    "        imshow(frame)\n",
    "        show()\n",
    "        # Display the frame until new frame is available\n",
    "        clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    # Release the Video Device\n",
    "    vid.release()\n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8fd08-9c24-4753-86f7-36014235cb73",
   "metadata": {},
   "source": [
    "The playing of video may slightly lagging.\\\n",
    "This is because of internet delay and the method refer is changing frame through looping the matplotlib graph showing function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b9bc7-7e11-43b8-bf14-a239df26b015",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Face side using face-net detection\n",
    "\n",
    "[Source](https://github.com/niconielsen32/ComputerVision/blob/master/headPoseEstimation.py)\\\n",
    "[Video referred](https://www.youtube.com/watch?v=-toNMaS4SeQ)\n",
    "\n",
    "This method applies face-net function provided in [MediaPipe](https://github.com/google/mediapipe). By extract the landmark from face and calculate the nose pointing position in (x,y).\n",
    "\n",
    "When Y less than -10 or 10 (Depends on defined thereshold) it will considered the face is facing left or right.\\\n",
    "When X less than -10 or 10 (Depends on defined thereshold) it will considered the face is facing down or up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff893da0-c405-471d-939b-3f491ae8d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:46:46.262670: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 00:46:47.232764: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azureuser/skt_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-03-08 00:46:47.232865: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azureuser/skt_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-03-08 00:46:47.232878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85fae3b3-1bbc-459f-bbbb-011af04488fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def facePoseInit():\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "    return face_mesh, mp_drawing, drawing_spec\n",
    "    \n",
    "\n",
    "def facePoseFunction(frame, thereshold, face_mesh_open, mp_drawing, drawing_spec):\n",
    "    # To improve performance\n",
    "    frame.flags.writeable = False\n",
    "    \n",
    "    # Get the result\n",
    "    results = face_mesh.process(frame)\n",
    "    \n",
    "    # To improve performance\n",
    "    frame.flags.writeable = True\n",
    "    \n",
    "    # Convert the color space from RGB to BGR\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = frame.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])       \n",
    "            \n",
    "            # Convert it to the NumPy array\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "\n",
    "            # Convert it to the NumPy array\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "            # The camera matrix\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
    "                                    [0, focal_length, img_w / 2],\n",
    "                                    [0, 0, 1]])\n",
    "\n",
    "            # The distortion parameters\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "            # Solve PnP\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "            # Get rotational matrix\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            # Get angles\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            # Get the y rotation degree\n",
    "            x = angles[0] * 360\n",
    "            y = angles[1] * 360\n",
    "            z = angles[2] * 360\n",
    "          \n",
    "\n",
    "            # See where the user's head tilting\n",
    "            if y < -thereshold:\n",
    "                text = \"Looking Left\"\n",
    "            elif y > thereshold:\n",
    "                text = \"Looking Right\"\n",
    "            elif x < -(thereshold*0.2):\n",
    "                text = \"Looking Down\"\n",
    "            elif x > thereshold:\n",
    "                text = \"Looking Up\"\n",
    "            else:\n",
    "                text = \"Forward\"\n",
    "\n",
    "            # Display the nose direction\n",
    "            nose_3d_projection, jacobian = cv2.projectPoints(nose_3d, rot_vec, trans_vec, cam_matrix, dist_matrix)\n",
    "\n",
    "            p1 = (int(nose_2d[0]), int(nose_2d[1]))\n",
    "            p2 = (int(nose_2d[0] + y * 10) , int(nose_2d[1] - x * 10))\n",
    "            \n",
    "            cv2.line(frame, p1, p2, (255, 0, 0), 3)\n",
    "\n",
    "            # Add the text on the frame\n",
    "            cv2.putText(frame, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"x: \" + str(np.round(x,2)), (500, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(frame, \"y: \" + str(np.round(y,2)), (500, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.putText(frame, \"z: \" + str(np.round(z,2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        totalTime = end - start\n",
    "\n",
    "        fps = 1 / totalTime\n",
    "        #print(\"FPS: \", fps)\n",
    "\n",
    "        cv2.putText(frame, f'FPS: {int(fps)}', (20,450), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "        \n",
    "        \n",
    "        if face_mesh_open is True:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                        image=frame,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        #connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                        landmark_drawing_spec=drawing_spec\n",
    "                        #connection_drawing_spec=drawing_spec\n",
    "            )\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ee33a2-e0f9-4101-ac40-7a6e6f90ccb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n"
     ]
    }
   ],
   "source": [
    "# video\n",
    "cap = cv2.VideoCapture(\"face-position-test-lowfps.mp4\")\n",
    "\n",
    "# webcam\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Put the code in try-except statements\n",
    "# Catch the keyboard exception and \n",
    "# release the camera device and \n",
    "# continue with the rest of code.\n",
    "try:\n",
    "    # init face side module\n",
    "    face_mesh, mp_drawing, drawing_spec = facePoseInit()\n",
    "    \n",
    "    \n",
    "    # Check if the webcam is opened correctly\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if not ret:\n",
    "            # Release the Video Device if ret is false\n",
    "            cap.release()\n",
    "            # Message to be displayed after releasing the device\n",
    "            print(\"Released Video Resource\")\n",
    "            break\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # flip the frame also\n",
    "        frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # frame, thereshold, show facenet?, mp_drawing, drawing_spec\n",
    "        # DONT TOUCH frame, mp_drawing, drawing_spec\n",
    "        frame = facePoseFunction(frame, 15 , True, mp_drawing, drawing_spec)\n",
    "        \n",
    "        \n",
    "        ### Codes for showing frame\n",
    "        # Turn off the axis\n",
    "        axis('off')\n",
    "        # Title of the window\n",
    "        title(\"Input Stream\")\n",
    "        # Display the frame\n",
    "        imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        show()\n",
    "        # Display the frame until new frame is available\n",
    "        # Overwrite the old graph\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# If stop button are clicked\n",
    "except KeyboardInterrupt:\n",
    "    # Release the Video Device\n",
    "    cap.release()\n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e419bf-84a4-4c41-8195-629d35789ad5",
   "metadata": {},
   "source": [
    "This ables to form two(2) liveness detection test:\n",
    "1. Turn the face to left and right.\n",
    "2. Nodding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f5e475-4202-4c04-9f1e-e1b8f8361781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faceDirectionFunction(frame, thereshold):\n",
    "    # To improve performance\n",
    "    frame.flags.writeable = False\n",
    "    \n",
    "    # Get the result\n",
    "    results = face_mesh.process(frame)\n",
    "    \n",
    "    # To improve performance\n",
    "    frame.flags.writeable = True\n",
    "    \n",
    "    # Convert the color space from RGB to BGR\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = frame.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])       \n",
    "            \n",
    "            # Convert it to the NumPy array\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "\n",
    "            # Convert it to the NumPy array\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "            # The camera matrix\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
    "                                    [0, focal_length, img_w / 2],\n",
    "                                    [0, 0, 1]])\n",
    "\n",
    "            # The distortion parameters\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "            # Solve PnP\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "            # Get rotational matrix\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            # Get angles\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            # Get the y rotation degree\n",
    "            x = angles[0] * 360\n",
    "            y = angles[1] * 360\n",
    "            z = angles[2] * 360\n",
    "          \n",
    "\n",
    "            # See where the user's head tilting\n",
    "            if y < -thereshold:\n",
    "                direction = \"left\"\n",
    "            elif y > thereshold:\n",
    "                direction = \"right\"\n",
    "            elif x < -(thereshold*0.2):\n",
    "                direction = \"down\"\n",
    "            elif x > thereshold:\n",
    "                direction = \"up\"\n",
    "            else:\n",
    "                direction = \"front\"\n",
    "        return direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8659e-1ba2-4ed9-951f-ca59833c50a6",
   "metadata": {},
   "source": [
    "Demostration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2133b833-7cb7-4aed-822f-dc6455c08168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n"
     ]
    }
   ],
   "source": [
    "# video\n",
    "cap = cv2.VideoCapture(\"face-direction-3.mp4\")\n",
    "# webcam\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Put the code in try-except statements\n",
    "# Catch the keyboard exception and \n",
    "# release the camera device and \n",
    "# continue with the rest of code.\n",
    "look_left = False\n",
    "look_right = False\n",
    "look_front = False\n",
    "look_down = False\n",
    "\n",
    "shake = ''\n",
    "nod = ''\n",
    "\n",
    "try:\n",
    "    # init face side module\n",
    "    face_mesh, mp_drawing, drawing_spec = facePoseInit()\n",
    "    \n",
    "    \n",
    "    # Check if the webcam is opened correctly\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "    while(True):\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if not ret:\n",
    "            # Release the Video Device if ret is false\n",
    "            cap.release()\n",
    "            # Message to be displayed after releasing the device\n",
    "            print(\"Released Video Resource\")\n",
    "            break\n",
    "        # Convert the image from OpenCV BGR format to matplotlib RGB format\n",
    "        # flip the frame also\n",
    "        frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # frame, thereshold\n",
    "        # DONT TOUCH frame\n",
    "        direction = faceDirectionFunction(frame, 5)\n",
    "        \n",
    "        if direction == \"left\":\n",
    "            if look_right == True:\n",
    "                shake = 'Passed'\n",
    "                look_left = False\n",
    "                look_right = False\n",
    "            look_left = True\n",
    "        \n",
    "        if direction == \"right\":\n",
    "            if look_left == True:\n",
    "                shake = 'Passed'\n",
    "                look_left = False\n",
    "                look_right = False\n",
    "            look_right = True\n",
    "        \n",
    "        if direction == \"front\":\n",
    "            look_front = True\n",
    "        \n",
    "        if direction == \"down\":\n",
    "            if look_front == True:\n",
    "                nod = 'Passed'\n",
    "                look_front = False\n",
    "                \n",
    "        \n",
    "        cv2.putText(frame, \"Head shaking test: {}\".format(shake), (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, \"Nod test: {}\".format(nod), (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, direction, (20, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        ### Codes for showing frame\n",
    "        # Turn off the axis\n",
    "        axis('off')\n",
    "        # Title of the window\n",
    "        title(\"Input Stream\")\n",
    "        # Display the frame\n",
    "        imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        show()\n",
    "        # Display the frame until new frame is available\n",
    "        # Overwrite the old graph\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# If stop button are clicked\n",
    "except KeyboardInterrupt:\n",
    "    # Release the Video Device\n",
    "    cap.release()\n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df202bb4-e163-41f5-a1a7-b2c28292c2f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Eye blinking using Landmark\n",
    "\n",
    "[Source Code](https://github.com/juan-csv/eye_blink_detection/blob/master/eye_blink_detection.py)\\\n",
    "[Article]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5127c9ed-65a7-448b-a700-a924d99cda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from imutils.video import FileVideoStream\n",
    "import cv2\n",
    "import time\n",
    "import f_detector\n",
    "import imutils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e7d735-cdcf-48f3-870c-d7bda68f20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n"
     ]
    }
   ],
   "source": [
    "# detector init\n",
    "detector = f_detector.eye_blink_detector()\n",
    "\n",
    "# counters\n",
    "COUNTER = 0\n",
    "TOTAL = 0\n",
    "\n",
    "# ----------------------------- video -----------------------------\n",
    "vid = FileVideoStream(\"./Zhi_Jie/blink-test-1.mp4\").start()\n",
    "\n",
    "try:\n",
    "    '''\n",
    "    # Check if the webcam is opened correctly\n",
    "    if not vid.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    '''\n",
    "    \n",
    "    while(True):\n",
    "        star_time = time.time()\n",
    "        im = vid.read()\n",
    "        im = cv2.flip(im, 1)\n",
    "        im = imutils.resize(im, width=720)\n",
    "        gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        # detectar_rostro    \n",
    "        rectangles = detector.detector_faces(gray, 0)\n",
    "        boxes_face = f_detector.convert_rectangles2array(rectangles,im)\n",
    "        if len(boxes_face)!=0:\n",
    "            # box on face\n",
    "            areas = f_detector.get_areas(boxes_face)\n",
    "            index = np.argmax(areas)\n",
    "            rectangles = rectangles[index]\n",
    "            boxes_face = np.expand_dims(boxes_face[index],axis=0)\n",
    "            # blinks_detector\n",
    "            COUNTER,TOTAL = detector.eye_blink(gray,rectangles,COUNTER,TOTAL)\n",
    "            # agregar bounding box\n",
    "            frame = f_detector.bounding_box(im,boxes_face,['blinks: {}'.format(TOTAL)])\n",
    "        else:\n",
    "            frame = im \n",
    "        # visualizacion \n",
    "        end_time = time.time() - star_time    \n",
    "        FPS = 1/end_time\n",
    "        cv2.putText(frame ,f\"FPS: {round(FPS,3)}\",(10,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,255),2)\n",
    "        # Turn off the axis\n",
    "        axis('off')\n",
    "        # Title of the window\n",
    "        title(\"Input Stream\")\n",
    "        # Display the frame\n",
    "        imshow(frame)\n",
    "        show()\n",
    "        # Display the frame until new frame is available\n",
    "        clear_output(wait=True)\n",
    "except KeyboardInterrupt:\n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96878f-59e6-4f8f-8494-4f0075f641a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Face expression detect (Smile)\n",
    "\n",
    "[Source](https://github.com/Furkan-Gulsen/face-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf4c8c94-6595-4d83-ab39-7787b009afbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "var code_show_err = false;\n",
       "var code_toggle_err = function() {\n",
       "    var stderrNodes = document.querySelectorAll('[data-mime-type=\"application/vnd.jupyter.stderr\"]')\n",
       "    var stderr = Array.from(stderrNodes)\n",
       "    if (code_show_err){\n",
       "        stderr.forEach(ele => ele.style.display = 'block');\n",
       "    } else {\n",
       "        stderr.forEach(ele => ele.style.display = 'none');\n",
       "    }\n",
       "    code_show_err = !code_show_err\n",
       "}\n",
       "document.addEventListener('DOMContentLoaded', code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a onclick=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "var code_show_err = false;\n",
    "var code_toggle_err = function() {\n",
    "    var stderrNodes = document.querySelectorAll('[data-mime-type=\"application/vnd.jupyter.stderr\"]')\n",
    "    var stderr = Array.from(stderrNodes)\n",
    "    if (code_show_err){\n",
    "        stderr.forEach(ele => ele.style.display = 'block');\n",
    "    } else {\n",
    "        stderr.forEach(ele => ele.style.display = 'none');\n",
    "    }\n",
    "    code_show_err = !code_show_err\n",
    "}\n",
    "document.addEventListener('DOMContentLoaded', code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a onclick=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e246c34a-c31f-4136-bcb1-8d5f3a6c09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78d1b1ac-eb71-496e-93db-c15cfe747897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:47:19.951023: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 00:47:20.637280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 50 MB memory:  -> device: 0, name: Tesla M60, pci bus id: 0001:00:00.0, compute capability: 5.2\n"
     ]
    }
   ],
   "source": [
    "emotion_offsets = (20, 40)\n",
    "emotions = {\n",
    "    0: {\n",
    "        \"emotion\": \"Angry\",\n",
    "        \"color\": (193, 69, 42)\n",
    "    },\n",
    "    1: {\n",
    "        \"emotion\": \"Disgust\",\n",
    "        \"color\": (164, 175, 49)\n",
    "    },\n",
    "    2: {\n",
    "        \"emotion\": \"Fear\",\n",
    "        \"color\": (40, 52, 155)\n",
    "    },\n",
    "    3: {\n",
    "        \"emotion\": \"Happy\",\n",
    "        \"color\": (23, 164, 28)\n",
    "    },\n",
    "    4: {\n",
    "        \"emotion\": \"Sad\",\n",
    "        \"color\": (164, 93, 23)\n",
    "    },\n",
    "    5: {\n",
    "        \"emotion\": \"Suprise\",\n",
    "        \"color\": (218, 229, 97)\n",
    "    },\n",
    "    6: {\n",
    "        \"emotion\": \"Neutral\",\n",
    "        \"color\": (108, 72, 200)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def shapePoints(shape):\n",
    "    coords = np.zeros((68, 2), dtype=\"int\")\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return coords\n",
    "\n",
    "\n",
    "def rectPoints(rect):\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "faceLandmarks = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(faceLandmarks)\n",
    "\n",
    "emotionModelPath = 'emotionModel.hdf5'  # fer2013_mini_XCEPTION.110-0.65\n",
    "emotionClassifier = load_model(emotionModelPath, compile=False)\n",
    "emotionTargetSize = emotionClassifier.input_shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a2ed46e-864f-42bd-a31f-8a1217068eb4",
   "metadata": {
    "gather": {
     "logged": 1678169403685
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 00:47:27.327693: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"_has_manual_control_dependencies\" value { b: true } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"Tesla M60\" frequency: 1177 num_cores: 16 environment { key: \"architecture\" value: \"5.2\" } environment { key: \"cuda\" value: \"11020\" } environment { key: \"cudnn\" value: \"8100\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 2097152 shared_memory_size_per_multiprocessor: 98304 memory_size: 52690944 bandwidth: 160320000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2023-03-08 00:47:27.397285: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-03-08 00:47:27.397359: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1152 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_1/conv2d_1/Conv2D' defined at (most recent call last):\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_27432/3370958959.py\", line 41, in <module>\n      emotion_prediction = emotionClassifier.predict(grayFace, verbose = 0)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model_1/conv2d_1/Conv2D'\nDNN library is not found.\n\t [[{{node model_1/conv2d_1/Conv2D}}]] [Op:__inference_predict_function_1442]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m grayFace \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(grayFace, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m grayFace \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(grayFace, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m emotion_prediction \u001b[38;5;241m=\u001b[39m \u001b[43memotionClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrayFace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m emotion_probability \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(emotion_prediction)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (emotion_probability \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.36\u001b[39m):\n",
      "File \u001b[0;32m~/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/skt_env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_1/conv2d_1/Conv2D' defined at (most recent call last):\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/anaconda/envs/azureml_py38/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_27432/3370958959.py\", line 41, in <module>\n      emotion_prediction = emotionClassifier.predict(grayFace, verbose = 0)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 283, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/azureuser/skt_env/lib/python3.8/site-packages/keras/layers/convolutional/base_conv.py\", line 255, in convolution_op\n      return tf.nn.convolution(\nNode: 'model_1/conv2d_1/Conv2D'\nDNN library is not found.\n\t [[{{node model_1/conv2d_1/Conv2D}}]] [Op:__inference_predict_function_1442]"
     ]
    }
   ],
   "source": [
    "# video\n",
    "cap = cv2.VideoCapture(\"vid.mp4\")\n",
    "\n",
    "# webcam\n",
    "#cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Put the code in try-except statements\n",
    "# Catch the keyboard exception and \n",
    "# release the camera device and \n",
    "# continue with the rest of code.\n",
    "try:\n",
    "    # Check if the webcam is opened correctly\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "    while(True):\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (768, 432))\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        grayFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(grayFrame, 0)\n",
    "        for rect in rects:\n",
    "            shape = predictor(grayFrame, rect)\n",
    "            points = shapePoints(shape)\n",
    "            (x, y, w, h) = rectPoints(rect)\n",
    "            grayFace = grayFrame[y:y + h, x:x + w]\n",
    "            try:\n",
    "                grayFace = cv2.resize(grayFace, (emotionTargetSize))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "            grayFace = grayFace.astype('float32')\n",
    "            grayFace = grayFace / 255.0\n",
    "            grayFace = (grayFace - 0.5) * 2.0\n",
    "            grayFace = np.expand_dims(grayFace, 0)\n",
    "            grayFace = np.expand_dims(grayFace, -1)\n",
    "            emotion_prediction = emotionClassifier.predict(grayFace, verbose = 0)\n",
    "            emotion_probability = np.max(emotion_prediction)\n",
    "            if (emotion_probability > 0.36):\n",
    "                emotion_label_arg = np.argmax(emotion_prediction)\n",
    "                color = emotions[emotion_label_arg]['color']\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.line(frame, (x, y + h), (x + 20, y + h + 20),\n",
    "                         color,\n",
    "                         thickness=2)\n",
    "                cv2.rectangle(frame, (x + 20, y + h + 20), (x + 110, y + h + 40),\n",
    "                              color, -1)\n",
    "                cv2.putText(frame, emotions[emotion_label_arg]['emotion'],\n",
    "                            (x + 25, y + h + 36), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                            (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            else:\n",
    "                color = (255, 255, 255)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        ### Codes for showing frame\n",
    "        # Turn off the axis\n",
    "        axis('off')\n",
    "        # Title of the window\n",
    "        title(\"Input Stream\")\n",
    "        # Display the frame\n",
    "        imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        show()\n",
    "        # Display the frame until new frame is available\n",
    "        # Overwrite the old graph\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# If stop button are clicked\n",
    "except KeyboardInterrupt:\n",
    "    # Release the Video Device\n",
    "    cap.release()\n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6759201-f1a0-4df7-893f-0e649c5a06af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mouth open detect\n",
    "\n",
    "[Source](https://github.com/mauckc/mouth-open/blob/master/detect_open_mouth.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae11935-a5e2-42e1-8dcd-77523ccdbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca25f5ef-0bd9-4110-9d4d-ccd9341bd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouth_aspect_ratio(mouth):\n",
    "\t# compute the euclidean distances between the two sets of\n",
    "\t# vertical mouth landmarks (x, y)-coordinates\n",
    "\tA = dist.euclidean(mouth[2], mouth[10]) # 51, 59\n",
    "\tB = dist.euclidean(mouth[4], mouth[8]) # 53, 57\n",
    "\n",
    "\t# compute the euclidean distance between the horizontal\n",
    "\t# mouth landmark (x, y)-coordinates\n",
    "\tC = dist.euclidean(mouth[0], mouth[6]) # 49, 55\n",
    "\n",
    "\t# compute the mouth aspect ratio\n",
    "\tmar = (A + B) / (2.0 * C)\n",
    "\n",
    "\t# return the mouth aspect ratio\n",
    "\treturn mar\n",
    "\n",
    "# define one constants, for mouth aspect ratio to indicate open mouth\n",
    "MOUTH_AR_THRESH = 0.79\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# grab the indexes of the facial landmarks for the mouth\n",
    "(mStart, mEnd) = (49, 68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628c8975-ffd0-4bb8-9f01-befa59f5488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n"
     ]
    }
   ],
   "source": [
    "# start the video stream thread\n",
    "vs = FileVideoStream(\"mouth.mp4\").start()\n",
    "\n",
    "frame_width = 640\n",
    "frame_height = 360\n",
    "\n",
    "# webcam\n",
    "#cap = VideoStream(0).start\n",
    "\n",
    "\n",
    "# Put the code in try-except statements\n",
    "# Catch the keyboard exception and \n",
    "# release the camera device and \n",
    "# continue with the rest of code.\n",
    "try:\n",
    "    \n",
    "    # loop over frames from the video stream\n",
    "    while True:\n",
    "        # grab the frame from the threaded video file stream, resize\n",
    "        # it, and convert it to grayscale\n",
    "        # channels)\n",
    "        frame = vs.read()\n",
    "        frame = imutils.resize(frame, width=640)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # detect faces in the grayscale frame\n",
    "        rects = detector(gray, 0)\n",
    "    \n",
    "        # loop over the face detections\n",
    "        for rect in rects:\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "            # array\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            # extract the mouth coordinates, then use the\n",
    "            # coordinates to compute the mouth aspect ratio\n",
    "            mouth = shape[mStart:mEnd]\n",
    "\n",
    "            mouthMAR = mouth_aspect_ratio(mouth)\n",
    "            mar = mouthMAR\n",
    "            # compute the convex hull for the mouth, then\n",
    "            # visualize the mouth\n",
    "            mouthHull = cv2.convexHull(mouth)\n",
    "\t\t\n",
    "            cv2.drawContours(frame, [mouthHull], -1, (0, 255, 0), 1)\n",
    "            cv2.putText(frame, \"MAR: {:.2f}\".format(mar), (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # Draw text if mouth is open\n",
    "            if mar > MOUTH_AR_THRESH:\n",
    "                cv2.putText(frame, \"Mouth is Open!\", (30,60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255),2)\n",
    "        # show the frame\n",
    "        #cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "        ### Codes for showing frame\n",
    "        # Turn off the axis\n",
    "        axis('off')\n",
    "        # Title of the window\n",
    "        title(\"Input Stream\")\n",
    "        # Display the frame\n",
    "        imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        show()\n",
    "        # Display the frame until new frame is available\n",
    "        # Overwrite the old graph\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# If stop button are clicked\n",
    "except KeyboardInterrupt:\n",
    "    # Release the Video Device\n",
    "    cap.release()\n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")\n",
    "except AttributeError:\n",
    "    # Message to be displayed after releasing the device\n",
    "    print(\"Released Video Resource\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "skt1"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
